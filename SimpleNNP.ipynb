{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ゼロから始める簡単NNP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNPとは？？\n",
    "Neural Network Potentialの略です。原子構造の情報を受け取って、エネルギーの値を予測する、ニューラルネットワークベースの機械学習モデルです。第一原理計算での構造とエネルギーの関係を再現するように、第一原理計算結果を使って学習させる仕組みを使っています。\n",
    "\n",
    "この最も単純なケースを試してみましょう。今回扱うのは、結晶Siです。分子動力学法のトラジェクトリーを使って、様々な温度での熱ゆらぎが加わっていたり、密度が異なった状態での1250パターンの構造とエネルギーの情報を集めてあるので、それを学習データにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 対称性関数の計算\n",
    "構造とエネルギーの関係を学ばせるにあたって、まず構造をニューラルネットワーク的に扱いやすいデータに変換する必要があります。\n",
    "その方法の一つが対称性関数です。詳細はRef[1]を見てもらうとして、ざっくりした説明としては、動径分布関数や角度の分布関数を工夫したようなものと考えておくと良いでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対称性関数はそこまでややこしい式ではないので、自力実装も可能ですが、近傍原子の判定などを効率よく書くのが難しいです。昔、私がpythonで自力で書いたものはとんでもなく遅かったので、最適化されているライブラリを使います。それが[DScribe](https://singroup.github.io/dscribe/latest/index.html)です。（最適化されているとはいえ、周期境界条件を入れると結構時間がかかるので、計算結果は保存しておくほうが効率が良いです）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dscribe.descriptors import ACSF\n",
    "\n",
    "nacsf=11\n",
    "# Setting up the ACSF descriptor\n",
    "acsf = ACSF(\n",
    "    species=[\"Si\"],\n",
    "    rcut=7.0,\n",
    "    g2_params=[[0.025,0.0], [0.05,0.0], [0.1,0.0],[0.2,0.0], [0.4,0.0],[0.8,0.0]],\n",
    "    g4_params=[[0.1, 1.0, 1], [0.1, 4.0, 1], [0.1, 1.0, -1], [0.1, 4.0, -1]],\n",
    "    periodic=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用意してある計算結果を読み込んで処理します。xyzという第一原理計算やMDの業界でよく使う形式で、座標やエネルギーの値をまとめてあります。これをASEというパッケージの機能を使って処理しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import iread\n",
    "xyzfile='sample50.xyz'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "atoms=iread(xyzfile,index=':', format='extxyz')\n",
    "nsample=1250\n",
    "desc=np.zeros((nsample,64,nacsf))\n",
    "label=np.zeros(nsample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ia, a in enumerate(atoms):\n",
    "\n",
    "\n",
    "    all=np.arange(len(a.positions))\n",
    "\n",
    "    acsf_Si = acsf.create(a)\n",
    "    desc[ia,:,:]=acsf_Si\n",
    "    label[ia]=a.get_total_energy()/64.0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 64, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データがどんな形か確認してみましょう。descは計算された対称性関数の8つの値（最初の値は、G0と表記されるシンプルな動径分布に対応します）を保持していて、labelは系の原子あたりの平均エネルギーの情報を持っています。(全エネルギーそのままだと値が大きすぎるので)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([13.57160854,  9.17270279,  6.44236135,  3.49307561,  1.35376608,\n",
       "         0.35307807,  0.03834928,  0.73605591,  0.43611127,  0.28210217,\n",
       "         0.05149077]),\n",
       " -5.812224315)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc[0,0,:], label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([14.42732716,  9.91098404,  7.07846117,  3.96392226,  1.62639427,\n",
       "         0.47494739,  0.06694016,  1.04516411,  0.59350401,  0.4443306 ,\n",
       "         0.08542389]),\n",
       " -5.812822678125)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc[1,0,:], label[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "対称性関数の値にもそれなりにばらつきがありそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このあと、ニューラルネットワークのフレームワークであるpytorchが扱いやすいように単精度に変換します。（機械学習フレームワークはだいたい単精度を使うことが多いです）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc=np.float32(desc)\n",
    "label=np.float32(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューラルネットワークの実装では配列の形を勘違いしてミスをすることが多いので確認しておきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1250, 64, 11), (1250,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使い回すために、numpyの機能でファイルに保存しておきます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('desc',desc)\n",
    "np.save('label',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本当はデータを正規化したほうが良いのですが、ひとまず単純に処理してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ニューラルネットワークの定義\n",
    " 今回は、ニューラルネットワークというと真っ先にイメージされる多層パーセプトロンを使います。\n",
    "\n",
    " このモデルでは、11の対称性関数の値を受け取って、それをノード数20の隠れ層に渡し、最終的に一つの出力を得る形になっています。\n",
    " この出力は「原子ひとつあたりのエネルギー」に相当すると考えます。原子ひとつあたりのエネルギーがきちんと定義できるのかはさておき、それらを足し上げると、系の全エネルギーになるようにネットワーク中の重みやバイアスを最適化していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "#対称性関数の入力は8次元、隠れ層のノード数は20、3層で出力は１のネットワーク\n",
    "\n",
    "class Net(nn.Module):\n",
    "    #n_sf : number of symmetry function\n",
    "    #two-hidden layer\n",
    "    #output is energy per atom\n",
    "    def __init__(self,n_sf,n_hidden):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(n_sf, n_hidden)\n",
    "        self.a1  = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(n_hidden, n_hidden)\n",
    "        self.a2  = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(n_hidden,1)\n",
    "\n",
    "        #for debug backprop\n",
    "        self.fc1mask=[]\n",
    "        self.fc2mask=[]\n",
    "\n",
    "        #He initialization\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.kaiming_normal_(self.fc3.weight)\n",
    "        self.results={}\n",
    "\n",
    "        self.layers=[self.fc1, self.a1, self.fc2, self.a2, self.fc3]\n",
    "\n",
    "    #relu actination function\n",
    "    #two hidden layer\n",
    "    #evaluate eneergy & derivative in forward run\n",
    "    def forward(self,x):\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x=layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習データとテストデータの分割\n",
    "機械学習では過学習（使ったデータにはよく合うが、それ以外のデータにはうまく対応できない）がつきものなので、汎化性能を調べるために、学習データとテストデータの分割を行います。今回は8割を学習データ、2割をテストデータにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(desc, label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorchはテンソルを入力として受け取り、出力もテンソルにすることができます。上記のseq_modelは8つの要素を持つベクトルを受け取ることを前提にかかれていますが、それが積み上がってテンソルになった入力に対しても柔軟に処理することができます。イメージ的に分かりにくいので、実際にやってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 64, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データ処理の概要確認\n",
    "#テンソルとしてデータを一気に流し込み、原子数の次元で和を取る\n",
    "model=Net(n_sf=nacsf,n_hidden=20)\n",
    "test_out=model(torch.tensor(X_train))\n",
    "test_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000データがあって、それぞれに（64,1）の配列が格納されている形式になっています。この64というのは原子数で、実際の正解データと比較するのは、64原子分の総和をとった値です。それは以下のようにして計算できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_energy=torch.sum(test_out, dim=1)\n",
    "p_energy.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習部分のコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはとてもシンプルにするために、各エポックで全部のデータについての予測値が出揃ってから更新をかけています。効率が悪いです。本来はミニバッチをつかって、更新頻度を高くして効率よく学習させます（データローダーを書くのが面倒なので暫定版）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, X_train, y_train, X_test, y_test):\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        tmp=model(X_train)\n",
    "        p_train=torch.sum(tmp, dim=1)/64.0\n",
    "\n",
    "        loss_train=loss_fn(p_train, y_train)\n",
    "        \n",
    "        p_val= torch.sum(model(X_test),dim=1)/64.0\n",
    "        loss_val=loss_fn(p_val, y_test)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch == 1 or epoch %10 ==0:\n",
    "            print('Epoch %d, Training Loss %f' %(epoch, loss_train))\n",
    "            print('\\t Validation Loss %f' %(loss_val))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torchのtensor型に変換します。y_train, y_testはモデルの出力と整合させるために、unsqueeze(1)で余分な次元をつけています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=torch.tensor(X_train)\n",
    "X_test=torch.tensor(X_test)\n",
    "y_train=torch.tensor(y_train).unsqueeze(1)\n",
    "y_test=torch.tensor(y_test).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss 0.000283\n",
      "\t Validation Loss 0.000310\n",
      "Epoch 10, Training Loss 0.000283\n",
      "\t Validation Loss 0.000310\n",
      "Epoch 20, Training Loss 0.000283\n",
      "\t Validation Loss 0.000310\n",
      "Epoch 30, Training Loss 0.000282\n",
      "\t Validation Loss 0.000309\n",
      "Epoch 40, Training Loss 0.000282\n",
      "\t Validation Loss 0.000309\n",
      "Epoch 50, Training Loss 0.000282\n",
      "\t Validation Loss 0.000309\n",
      "Epoch 60, Training Loss 0.000309\n",
      "\t Validation Loss 0.000330\n",
      "Epoch 70, Training Loss 0.000294\n",
      "\t Validation Loss 0.000315\n",
      "Epoch 80, Training Loss 0.000307\n",
      "\t Validation Loss 0.000325\n",
      "Epoch 90, Training Loss 0.000286\n",
      "\t Validation Loss 0.000308\n",
      "Epoch 100, Training Loss 0.000282\n",
      "\t Validation Loss 0.000307\n",
      "Epoch 110, Training Loss 0.000281\n",
      "\t Validation Loss 0.000307\n",
      "Epoch 120, Training Loss 0.000281\n",
      "\t Validation Loss 0.000308\n",
      "Epoch 130, Training Loss 0.000281\n",
      "\t Validation Loss 0.000307\n",
      "Epoch 140, Training Loss 0.000280\n",
      "\t Validation Loss 0.000307\n",
      "Epoch 150, Training Loss 0.000280\n",
      "\t Validation Loss 0.000307\n",
      "Epoch 160, Training Loss 0.000280\n",
      "\t Validation Loss 0.000307\n",
      "Epoch 170, Training Loss 0.000280\n",
      "\t Validation Loss 0.000306\n",
      "Epoch 180, Training Loss 0.000279\n",
      "\t Validation Loss 0.000306\n",
      "Epoch 190, Training Loss 0.000279\n",
      "\t Validation Loss 0.000306\n",
      "Epoch 200, Training Loss 0.000279\n",
      "\t Validation Loss 0.000306\n",
      "Epoch 210, Training Loss 0.000279\n",
      "\t Validation Loss 0.000305\n",
      "Epoch 220, Training Loss 0.000279\n",
      "\t Validation Loss 0.000305\n",
      "Epoch 230, Training Loss 0.000278\n",
      "\t Validation Loss 0.000305\n",
      "Epoch 240, Training Loss 0.000278\n",
      "\t Validation Loss 0.000305\n",
      "Epoch 250, Training Loss 0.000278\n",
      "\t Validation Loss 0.000305\n",
      "Epoch 260, Training Loss 0.000278\n",
      "\t Validation Loss 0.000304\n",
      "Epoch 270, Training Loss 0.000278\n",
      "\t Validation Loss 0.000304\n",
      "Epoch 280, Training Loss 0.000277\n",
      "\t Validation Loss 0.000304\n",
      "Epoch 290, Training Loss 0.000277\n",
      "\t Validation Loss 0.000304\n",
      "Epoch 300, Training Loss 0.000277\n",
      "\t Validation Loss 0.000304\n",
      "Epoch 310, Training Loss 0.000277\n",
      "\t Validation Loss 0.000304\n",
      "Epoch 320, Training Loss 0.000278\n",
      "\t Validation Loss 0.000306\n",
      "Epoch 330, Training Loss 0.000372\n",
      "\t Validation Loss 0.000408\n",
      "Epoch 340, Training Loss 0.000315\n",
      "\t Validation Loss 0.000332\n",
      "Epoch 350, Training Loss 0.000293\n",
      "\t Validation Loss 0.000322\n",
      "Epoch 360, Training Loss 0.000283\n",
      "\t Validation Loss 0.000311\n",
      "Epoch 370, Training Loss 0.000277\n",
      "\t Validation Loss 0.000302\n",
      "Epoch 380, Training Loss 0.000276\n",
      "\t Validation Loss 0.000302\n",
      "Epoch 390, Training Loss 0.000276\n",
      "\t Validation Loss 0.000303\n",
      "Epoch 400, Training Loss 0.000275\n",
      "\t Validation Loss 0.000301\n",
      "Epoch 410, Training Loss 0.000275\n",
      "\t Validation Loss 0.000301\n",
      "Epoch 420, Training Loss 0.000275\n",
      "\t Validation Loss 0.000301\n",
      "Epoch 430, Training Loss 0.000274\n",
      "\t Validation Loss 0.000301\n",
      "Epoch 440, Training Loss 0.000274\n",
      "\t Validation Loss 0.000300\n",
      "Epoch 450, Training Loss 0.000274\n",
      "\t Validation Loss 0.000300\n",
      "Epoch 460, Training Loss 0.000274\n",
      "\t Validation Loss 0.000300\n",
      "Epoch 470, Training Loss 0.000273\n",
      "\t Validation Loss 0.000300\n",
      "Epoch 480, Training Loss 0.000273\n",
      "\t Validation Loss 0.000300\n",
      "Epoch 490, Training Loss 0.000273\n",
      "\t Validation Loss 0.000299\n",
      "Epoch 500, Training Loss 0.000273\n",
      "\t Validation Loss 0.000299\n",
      "Epoch 510, Training Loss 0.000276\n",
      "\t Validation Loss 0.000304\n",
      "Epoch 520, Training Loss 0.000507\n",
      "\t Validation Loss 0.000548\n",
      "Epoch 530, Training Loss 0.000343\n",
      "\t Validation Loss 0.000357\n",
      "Epoch 540, Training Loss 0.000308\n",
      "\t Validation Loss 0.000324\n",
      "Epoch 550, Training Loss 0.000282\n",
      "\t Validation Loss 0.000303\n",
      "Epoch 560, Training Loss 0.000272\n",
      "\t Validation Loss 0.000297\n",
      "Epoch 570, Training Loss 0.000272\n",
      "\t Validation Loss 0.000300\n",
      "Epoch 580, Training Loss 0.000272\n",
      "\t Validation Loss 0.000299\n",
      "Epoch 590, Training Loss 0.000271\n",
      "\t Validation Loss 0.000298\n",
      "Epoch 600, Training Loss 0.000271\n",
      "\t Validation Loss 0.000297\n",
      "Epoch 610, Training Loss 0.000271\n",
      "\t Validation Loss 0.000297\n",
      "Epoch 620, Training Loss 0.000271\n",
      "\t Validation Loss 0.000297\n",
      "Epoch 630, Training Loss 0.000270\n",
      "\t Validation Loss 0.000296\n",
      "Epoch 640, Training Loss 0.000270\n",
      "\t Validation Loss 0.000296\n",
      "Epoch 650, Training Loss 0.000270\n",
      "\t Validation Loss 0.000296\n",
      "Epoch 660, Training Loss 0.000270\n",
      "\t Validation Loss 0.000296\n",
      "Epoch 670, Training Loss 0.000270\n",
      "\t Validation Loss 0.000296\n",
      "Epoch 680, Training Loss 0.000269\n",
      "\t Validation Loss 0.000295\n",
      "Epoch 690, Training Loss 0.000269\n",
      "\t Validation Loss 0.000295\n",
      "Epoch 700, Training Loss 0.000269\n",
      "\t Validation Loss 0.000295\n",
      "Epoch 710, Training Loss 0.000269\n",
      "\t Validation Loss 0.000295\n",
      "Epoch 720, Training Loss 0.000269\n",
      "\t Validation Loss 0.000294\n",
      "Epoch 730, Training Loss 0.000269\n",
      "\t Validation Loss 0.000294\n",
      "Epoch 740, Training Loss 0.000284\n",
      "\t Validation Loss 0.000306\n",
      "Epoch 750, Training Loss 0.000435\n",
      "\t Validation Loss 0.000445\n",
      "Epoch 760, Training Loss 0.000277\n",
      "\t Validation Loss 0.000297\n",
      "Epoch 770, Training Loss 0.000275\n",
      "\t Validation Loss 0.000302\n",
      "Epoch 780, Training Loss 0.000275\n",
      "\t Validation Loss 0.000303\n",
      "Epoch 790, Training Loss 0.000267\n",
      "\t Validation Loss 0.000293\n",
      "Epoch 800, Training Loss 0.000268\n",
      "\t Validation Loss 0.000293\n",
      "Epoch 810, Training Loss 0.000267\n",
      "\t Validation Loss 0.000294\n",
      "Epoch 820, Training Loss 0.000267\n",
      "\t Validation Loss 0.000293\n",
      "Epoch 830, Training Loss 0.000267\n",
      "\t Validation Loss 0.000292\n",
      "Epoch 840, Training Loss 0.000266\n",
      "\t Validation Loss 0.000292\n",
      "Epoch 850, Training Loss 0.000266\n",
      "\t Validation Loss 0.000292\n",
      "Epoch 860, Training Loss 0.000266\n",
      "\t Validation Loss 0.000292\n",
      "Epoch 870, Training Loss 0.000266\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 880, Training Loss 0.000266\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 890, Training Loss 0.000265\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 900, Training Loss 0.000265\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 910, Training Loss 0.000265\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 920, Training Loss 0.000265\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 930, Training Loss 0.000265\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 940, Training Loss 0.000287\n",
      "\t Validation Loss 0.000318\n",
      "Epoch 950, Training Loss 0.000351\n",
      "\t Validation Loss 0.000385\n",
      "Epoch 960, Training Loss 0.000304\n",
      "\t Validation Loss 0.000334\n",
      "Epoch 970, Training Loss 0.000265\n",
      "\t Validation Loss 0.000289\n",
      "Epoch 980, Training Loss 0.000268\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 990, Training Loss 0.000265\n",
      "\t Validation Loss 0.000290\n",
      "Epoch 1000, Training Loss 0.000264\n",
      "\t Validation Loss 0.000290\n",
      "Epoch 1010, Training Loss 0.000263\n",
      "\t Validation Loss 0.000290\n",
      "Epoch 1020, Training Loss 0.000263\n",
      "\t Validation Loss 0.000288\n",
      "Epoch 1030, Training Loss 0.000263\n",
      "\t Validation Loss 0.000289\n",
      "Epoch 1040, Training Loss 0.000263\n",
      "\t Validation Loss 0.000288\n",
      "Epoch 1050, Training Loss 0.000262\n",
      "\t Validation Loss 0.000288\n",
      "Epoch 1060, Training Loss 0.000262\n",
      "\t Validation Loss 0.000288\n",
      "Epoch 1070, Training Loss 0.000262\n",
      "\t Validation Loss 0.000287\n",
      "Epoch 1080, Training Loss 0.000262\n",
      "\t Validation Loss 0.000287\n",
      "Epoch 1090, Training Loss 0.000262\n",
      "\t Validation Loss 0.000287\n",
      "Epoch 1100, Training Loss 0.000262\n",
      "\t Validation Loss 0.000287\n",
      "Epoch 1110, Training Loss 0.000261\n",
      "\t Validation Loss 0.000287\n",
      "Epoch 1120, Training Loss 0.000261\n",
      "\t Validation Loss 0.000286\n",
      "Epoch 1130, Training Loss 0.000261\n",
      "\t Validation Loss 0.000286\n",
      "Epoch 1140, Training Loss 0.000269\n",
      "\t Validation Loss 0.000291\n",
      "Epoch 1150, Training Loss 0.000484\n",
      "\t Validation Loss 0.000491\n",
      "Epoch 1160, Training Loss 0.000278\n",
      "\t Validation Loss 0.000305\n",
      "Epoch 1170, Training Loss 0.000281\n",
      "\t Validation Loss 0.000310\n",
      "Epoch 1180, Training Loss 0.000260\n",
      "\t Validation Loss 0.000285\n",
      "Epoch 1190, Training Loss 0.000263\n",
      "\t Validation Loss 0.000286\n",
      "Epoch 1200, Training Loss 0.000260\n",
      "\t Validation Loss 0.000286\n",
      "Epoch 1210, Training Loss 0.000260\n",
      "\t Validation Loss 0.000286\n",
      "Epoch 1220, Training Loss 0.000259\n",
      "\t Validation Loss 0.000284\n",
      "Epoch 1230, Training Loss 0.000259\n",
      "\t Validation Loss 0.000285\n",
      "Epoch 1240, Training Loss 0.000259\n",
      "\t Validation Loss 0.000284\n",
      "Epoch 1250, Training Loss 0.000259\n",
      "\t Validation Loss 0.000284\n",
      "Epoch 1260, Training Loss 0.000259\n",
      "\t Validation Loss 0.000284\n",
      "Epoch 1270, Training Loss 0.000258\n",
      "\t Validation Loss 0.000283\n",
      "Epoch 1280, Training Loss 0.000258\n",
      "\t Validation Loss 0.000283\n",
      "Epoch 1290, Training Loss 0.000258\n",
      "\t Validation Loss 0.000283\n",
      "Epoch 1300, Training Loss 0.000258\n",
      "\t Validation Loss 0.000283\n",
      "Epoch 1310, Training Loss 0.000258\n",
      "\t Validation Loss 0.000283\n",
      "Epoch 1320, Training Loss 0.000258\n",
      "\t Validation Loss 0.000283\n",
      "Epoch 1330, Training Loss 0.000262\n",
      "\t Validation Loss 0.000289\n",
      "Epoch 1340, Training Loss 0.000506\n",
      "\t Validation Loss 0.000547\n",
      "Epoch 1350, Training Loss 0.000293\n",
      "\t Validation Loss 0.000309\n",
      "Epoch 1360, Training Loss 0.000286\n",
      "\t Validation Loss 0.000303\n",
      "Epoch 1370, Training Loss 0.000265\n",
      "\t Validation Loss 0.000286\n",
      "Epoch 1380, Training Loss 0.000256\n",
      "\t Validation Loss 0.000281\n",
      "Epoch 1390, Training Loss 0.000258\n",
      "\t Validation Loss 0.000284\n",
      "Epoch 1400, Training Loss 0.000256\n",
      "\t Validation Loss 0.000282\n",
      "Epoch 1410, Training Loss 0.000256\n",
      "\t Validation Loss 0.000281\n",
      "Epoch 1420, Training Loss 0.000256\n",
      "\t Validation Loss 0.000281\n",
      "Epoch 1430, Training Loss 0.000256\n",
      "\t Validation Loss 0.000280\n",
      "Epoch 1440, Training Loss 0.000255\n",
      "\t Validation Loss 0.000280\n",
      "Epoch 1450, Training Loss 0.000255\n",
      "\t Validation Loss 0.000280\n",
      "Epoch 1460, Training Loss 0.000255\n",
      "\t Validation Loss 0.000280\n",
      "Epoch 1470, Training Loss 0.000255\n",
      "\t Validation Loss 0.000279\n",
      "Epoch 1480, Training Loss 0.000255\n",
      "\t Validation Loss 0.000279\n",
      "Epoch 1490, Training Loss 0.000254\n",
      "\t Validation Loss 0.000279\n",
      "Epoch 1500, Training Loss 0.000254\n",
      "\t Validation Loss 0.000279\n",
      "Epoch 1510, Training Loss 0.000254\n",
      "\t Validation Loss 0.000279\n",
      "Epoch 1520, Training Loss 0.000254\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 1530, Training Loss 0.000254\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 1540, Training Loss 0.000254\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 1550, Training Loss 0.000260\n",
      "\t Validation Loss 0.000288\n",
      "Epoch 1560, Training Loss 0.000528\n",
      "\t Validation Loss 0.000569\n",
      "Epoch 1570, Training Loss 0.000254\n",
      "\t Validation Loss 0.000276\n",
      "Epoch 1580, Training Loss 0.000261\n",
      "\t Validation Loss 0.000280\n",
      "Epoch 1590, Training Loss 0.000261\n",
      "\t Validation Loss 0.000282\n",
      "Epoch 1600, Training Loss 0.000255\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 1610, Training Loss 0.000253\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 1620, Training Loss 0.000253\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 1630, Training Loss 0.000252\n",
      "\t Validation Loss 0.000276\n",
      "Epoch 1640, Training Loss 0.000252\n",
      "\t Validation Loss 0.000276\n",
      "Epoch 1650, Training Loss 0.000252\n",
      "\t Validation Loss 0.000276\n",
      "Epoch 1660, Training Loss 0.000252\n",
      "\t Validation Loss 0.000276\n",
      "Epoch 1670, Training Loss 0.000251\n",
      "\t Validation Loss 0.000276\n",
      "Epoch 1680, Training Loss 0.000251\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 1690, Training Loss 0.000251\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 1700, Training Loss 0.000251\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 1710, Training Loss 0.000251\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 1720, Training Loss 0.000250\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 1730, Training Loss 0.000250\n",
      "\t Validation Loss 0.000274\n",
      "Epoch 1740, Training Loss 0.000250\n",
      "\t Validation Loss 0.000274\n",
      "Epoch 1750, Training Loss 0.000250\n",
      "\t Validation Loss 0.000274\n",
      "Epoch 1760, Training Loss 0.000250\n",
      "\t Validation Loss 0.000274\n",
      "Epoch 1770, Training Loss 0.000257\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 1780, Training Loss 0.000502\n",
      "\t Validation Loss 0.000507\n",
      "Epoch 1790, Training Loss 0.000257\n",
      "\t Validation Loss 0.000282\n",
      "Epoch 1800, Training Loss 0.000270\n",
      "\t Validation Loss 0.000298\n",
      "Epoch 1810, Training Loss 0.000255\n",
      "\t Validation Loss 0.000281\n",
      "Epoch 1820, Training Loss 0.000249\n",
      "\t Validation Loss 0.000273\n",
      "Epoch 1830, Training Loss 0.000250\n",
      "\t Validation Loss 0.000273\n",
      "Epoch 1840, Training Loss 0.000249\n",
      "\t Validation Loss 0.000274\n",
      "Epoch 1850, Training Loss 0.000248\n",
      "\t Validation Loss 0.000272\n",
      "Epoch 1860, Training Loss 0.000248\n",
      "\t Validation Loss 0.000272\n",
      "Epoch 1870, Training Loss 0.000248\n",
      "\t Validation Loss 0.000272\n",
      "Epoch 1880, Training Loss 0.000248\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 1890, Training Loss 0.000248\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 1900, Training Loss 0.000247\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 1910, Training Loss 0.000247\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 1920, Training Loss 0.000247\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 1930, Training Loss 0.000247\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 1940, Training Loss 0.000247\n",
      "\t Validation Loss 0.000270\n",
      "Epoch 1950, Training Loss 0.000247\n",
      "\t Validation Loss 0.000270\n",
      "Epoch 1960, Training Loss 0.000246\n",
      "\t Validation Loss 0.000270\n",
      "Epoch 1970, Training Loss 0.000250\n",
      "\t Validation Loss 0.000276\n",
      "Epoch 1980, Training Loss 0.000444\n",
      "\t Validation Loss 0.000482\n",
      "Epoch 1990, Training Loss 0.000311\n",
      "\t Validation Loss 0.000323\n",
      "Epoch 2000, Training Loss 0.000265\n",
      "\t Validation Loss 0.000282\n",
      "Epoch 2010, Training Loss 0.000246\n",
      "\t Validation Loss 0.000269\n",
      "Epoch 2020, Training Loss 0.000249\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 2030, Training Loss 0.000245\n",
      "\t Validation Loss 0.000269\n",
      "Epoch 2040, Training Loss 0.000245\n",
      "\t Validation Loss 0.000269\n",
      "Epoch 2050, Training Loss 0.000245\n",
      "\t Validation Loss 0.000269\n",
      "Epoch 2060, Training Loss 0.000245\n",
      "\t Validation Loss 0.000268\n",
      "Epoch 2070, Training Loss 0.000245\n",
      "\t Validation Loss 0.000268\n",
      "Epoch 2080, Training Loss 0.000244\n",
      "\t Validation Loss 0.000268\n",
      "Epoch 2090, Training Loss 0.000244\n",
      "\t Validation Loss 0.000267\n",
      "Epoch 2100, Training Loss 0.000244\n",
      "\t Validation Loss 0.000267\n",
      "Epoch 2110, Training Loss 0.000244\n",
      "\t Validation Loss 0.000267\n",
      "Epoch 2120, Training Loss 0.000244\n",
      "\t Validation Loss 0.000267\n",
      "Epoch 2130, Training Loss 0.000244\n",
      "\t Validation Loss 0.000267\n",
      "Epoch 2140, Training Loss 0.000243\n",
      "\t Validation Loss 0.000266\n",
      "Epoch 2150, Training Loss 0.000243\n",
      "\t Validation Loss 0.000266\n",
      "Epoch 2160, Training Loss 0.000243\n",
      "\t Validation Loss 0.000266\n",
      "Epoch 2170, Training Loss 0.000244\n",
      "\t Validation Loss 0.000268\n",
      "Epoch 2180, Training Loss 0.000347\n",
      "\t Validation Loss 0.000381\n",
      "Epoch 2190, Training Loss 0.000353\n",
      "\t Validation Loss 0.000361\n",
      "Epoch 2200, Training Loss 0.000259\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 2210, Training Loss 0.000245\n",
      "\t Validation Loss 0.000265\n",
      "Epoch 2220, Training Loss 0.000242\n",
      "\t Validation Loss 0.000266\n",
      "Epoch 2230, Training Loss 0.000244\n",
      "\t Validation Loss 0.000269\n",
      "Epoch 2240, Training Loss 0.000242\n",
      "\t Validation Loss 0.000266\n",
      "Epoch 2250, Training Loss 0.000242\n",
      "\t Validation Loss 0.000264\n",
      "Epoch 2260, Training Loss 0.000241\n",
      "\t Validation Loss 0.000264\n",
      "Epoch 2270, Training Loss 0.000241\n",
      "\t Validation Loss 0.000264\n",
      "Epoch 2280, Training Loss 0.000241\n",
      "\t Validation Loss 0.000264\n",
      "Epoch 2290, Training Loss 0.000241\n",
      "\t Validation Loss 0.000264\n",
      "Epoch 2300, Training Loss 0.000241\n",
      "\t Validation Loss 0.000263\n",
      "Epoch 2310, Training Loss 0.000241\n",
      "\t Validation Loss 0.000263\n",
      "Epoch 2320, Training Loss 0.000240\n",
      "\t Validation Loss 0.000263\n",
      "Epoch 2330, Training Loss 0.000240\n",
      "\t Validation Loss 0.000263\n",
      "Epoch 2340, Training Loss 0.000240\n",
      "\t Validation Loss 0.000263\n",
      "Epoch 2350, Training Loss 0.000240\n",
      "\t Validation Loss 0.000263\n",
      "Epoch 2360, Training Loss 0.000240\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2370, Training Loss 0.000240\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2380, Training Loss 0.000239\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2390, Training Loss 0.000239\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2400, Training Loss 0.000239\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2410, Training Loss 0.000243\n",
      "\t Validation Loss 0.000267\n",
      "Epoch 2420, Training Loss 0.000505\n",
      "\t Validation Loss 0.000544\n",
      "Epoch 2430, Training Loss 0.000257\n",
      "\t Validation Loss 0.000272\n",
      "Epoch 2440, Training Loss 0.000256\n",
      "\t Validation Loss 0.000272\n",
      "Epoch 2450, Training Loss 0.000249\n",
      "\t Validation Loss 0.000267\n",
      "Epoch 2460, Training Loss 0.000242\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2470, Training Loss 0.000238\n",
      "\t Validation Loss 0.000261\n",
      "Epoch 2480, Training Loss 0.000239\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2490, Training Loss 0.000238\n",
      "\t Validation Loss 0.000260\n",
      "Epoch 2500, Training Loss 0.000238\n",
      "\t Validation Loss 0.000260\n",
      "Epoch 2510, Training Loss 0.000237\n",
      "\t Validation Loss 0.000260\n",
      "Epoch 2520, Training Loss 0.000237\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 2530, Training Loss 0.000237\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 2540, Training Loss 0.000237\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 2550, Training Loss 0.000237\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 2560, Training Loss 0.000237\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 2570, Training Loss 0.000237\n",
      "\t Validation Loss 0.000258\n",
      "Epoch 2580, Training Loss 0.000236\n",
      "\t Validation Loss 0.000258\n",
      "Epoch 2590, Training Loss 0.000236\n",
      "\t Validation Loss 0.000258\n",
      "Epoch 2600, Training Loss 0.000236\n",
      "\t Validation Loss 0.000258\n",
      "Epoch 2610, Training Loss 0.000236\n",
      "\t Validation Loss 0.000258\n",
      "Epoch 2620, Training Loss 0.000236\n",
      "\t Validation Loss 0.000257\n",
      "Epoch 2630, Training Loss 0.000236\n",
      "\t Validation Loss 0.000257\n",
      "Epoch 2640, Training Loss 0.000243\n",
      "\t Validation Loss 0.000262\n",
      "Epoch 2650, Training Loss 0.000498\n",
      "\t Validation Loss 0.000501\n",
      "Epoch 2660, Training Loss 0.000237\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2670, Training Loss 0.000241\n",
      "\t Validation Loss 0.000264\n",
      "Epoch 2680, Training Loss 0.000243\n",
      "\t Validation Loss 0.000268\n",
      "Epoch 2690, Training Loss 0.000236\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 2700, Training Loss 0.000235\n",
      "\t Validation Loss 0.000257\n",
      "Epoch 2710, Training Loss 0.000234\n",
      "\t Validation Loss 0.000256\n",
      "Epoch 2720, Training Loss 0.000234\n",
      "\t Validation Loss 0.000256\n",
      "Epoch 2730, Training Loss 0.000234\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2740, Training Loss 0.000234\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2750, Training Loss 0.000234\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2760, Training Loss 0.000234\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2770, Training Loss 0.000233\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2780, Training Loss 0.000233\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2790, Training Loss 0.000233\n",
      "\t Validation Loss 0.000254\n",
      "Epoch 2800, Training Loss 0.000233\n",
      "\t Validation Loss 0.000254\n",
      "Epoch 2810, Training Loss 0.000233\n",
      "\t Validation Loss 0.000254\n",
      "Epoch 2820, Training Loss 0.000233\n",
      "\t Validation Loss 0.000254\n",
      "Epoch 2830, Training Loss 0.000233\n",
      "\t Validation Loss 0.000254\n",
      "Epoch 2840, Training Loss 0.000232\n",
      "\t Validation Loss 0.000254\n",
      "Epoch 2850, Training Loss 0.000236\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 2860, Training Loss 0.000448\n",
      "\t Validation Loss 0.000484\n",
      "Epoch 2870, Training Loss 0.000278\n",
      "\t Validation Loss 0.000290\n",
      "Epoch 2880, Training Loss 0.000261\n",
      "\t Validation Loss 0.000274\n",
      "Epoch 2890, Training Loss 0.000237\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2900, Training Loss 0.000232\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2910, Training Loss 0.000232\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 2920, Training Loss 0.000232\n",
      "\t Validation Loss 0.000252\n",
      "Epoch 2930, Training Loss 0.000231\n",
      "\t Validation Loss 0.000252\n",
      "Epoch 2940, Training Loss 0.000231\n",
      "\t Validation Loss 0.000252\n",
      "Epoch 2950, Training Loss 0.000231\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 2960, Training Loss 0.000231\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 2970, Training Loss 0.000230\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 2980, Training Loss 0.000230\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 2990, Training Loss 0.000230\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 3000, Training Loss 0.000230\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 3010, Training Loss 0.000230\n",
      "\t Validation Loss 0.000250\n",
      "Epoch 3020, Training Loss 0.000230\n",
      "\t Validation Loss 0.000250\n",
      "Epoch 3030, Training Loss 0.000230\n",
      "\t Validation Loss 0.000250\n",
      "Epoch 3040, Training Loss 0.000230\n",
      "\t Validation Loss 0.000250\n",
      "Epoch 3050, Training Loss 0.000234\n",
      "\t Validation Loss 0.000256\n",
      "Epoch 3060, Training Loss 0.000398\n",
      "\t Validation Loss 0.000432\n",
      "Epoch 3070, Training Loss 0.000288\n",
      "\t Validation Loss 0.000298\n",
      "Epoch 3080, Training Loss 0.000237\n",
      "\t Validation Loss 0.000253\n",
      "Epoch 3090, Training Loss 0.000233\n",
      "\t Validation Loss 0.000256\n",
      "Epoch 3100, Training Loss 0.000229\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 3110, Training Loss 0.000230\n",
      "\t Validation Loss 0.000249\n",
      "Epoch 3120, Training Loss 0.000229\n",
      "\t Validation Loss 0.000250\n",
      "Epoch 3130, Training Loss 0.000228\n",
      "\t Validation Loss 0.000248\n",
      "Epoch 3140, Training Loss 0.000228\n",
      "\t Validation Loss 0.000248\n",
      "Epoch 3150, Training Loss 0.000228\n",
      "\t Validation Loss 0.000248\n",
      "Epoch 3160, Training Loss 0.000228\n",
      "\t Validation Loss 0.000248\n",
      "Epoch 3170, Training Loss 0.000227\n",
      "\t Validation Loss 0.000248\n",
      "Epoch 3180, Training Loss 0.000227\n",
      "\t Validation Loss 0.000247\n",
      "Epoch 3190, Training Loss 0.000227\n",
      "\t Validation Loss 0.000247\n",
      "Epoch 3200, Training Loss 0.000227\n",
      "\t Validation Loss 0.000247\n",
      "Epoch 3210, Training Loss 0.000227\n",
      "\t Validation Loss 0.000247\n",
      "Epoch 3220, Training Loss 0.000227\n",
      "\t Validation Loss 0.000247\n",
      "Epoch 3230, Training Loss 0.000232\n",
      "\t Validation Loss 0.000249\n",
      "Epoch 3240, Training Loss 0.000499\n",
      "\t Validation Loss 0.000499\n",
      "Epoch 3250, Training Loss 0.000230\n",
      "\t Validation Loss 0.000249\n",
      "Epoch 3260, Training Loss 0.000237\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 3270, Training Loss 0.000236\n",
      "\t Validation Loss 0.000259\n",
      "Epoch 3280, Training Loss 0.000227\n",
      "\t Validation Loss 0.000249\n",
      "Epoch 3290, Training Loss 0.000227\n",
      "\t Validation Loss 0.000246\n",
      "Epoch 3300, Training Loss 0.000226\n",
      "\t Validation Loss 0.000245\n",
      "Epoch 3310, Training Loss 0.000226\n",
      "\t Validation Loss 0.000246\n",
      "Epoch 3320, Training Loss 0.000225\n",
      "\t Validation Loss 0.000245\n",
      "Epoch 3330, Training Loss 0.000225\n",
      "\t Validation Loss 0.000245\n",
      "Epoch 3340, Training Loss 0.000225\n",
      "\t Validation Loss 0.000245\n",
      "Epoch 3350, Training Loss 0.000225\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 3360, Training Loss 0.000225\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 3370, Training Loss 0.000225\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 3380, Training Loss 0.000225\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 3390, Training Loss 0.000224\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 3400, Training Loss 0.000224\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 3410, Training Loss 0.000224\n",
      "\t Validation Loss 0.000243\n",
      "Epoch 3420, Training Loss 0.000224\n",
      "\t Validation Loss 0.000243\n",
      "Epoch 3430, Training Loss 0.000224\n",
      "\t Validation Loss 0.000243\n",
      "Epoch 3440, Training Loss 0.000224\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 3450, Training Loss 0.000276\n",
      "\t Validation Loss 0.000303\n",
      "Epoch 3460, Training Loss 0.000237\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 3470, Training Loss 0.000232\n",
      "\t Validation Loss 0.000253\n",
      "Epoch 3480, Training Loss 0.000235\n",
      "\t Validation Loss 0.000257\n",
      "Epoch 3490, Training Loss 0.000228\n",
      "\t Validation Loss 0.000250\n",
      "Epoch 3500, Training Loss 0.000223\n",
      "\t Validation Loss 0.000242\n",
      "Epoch 3510, Training Loss 0.000223\n",
      "\t Validation Loss 0.000242\n",
      "Epoch 3520, Training Loss 0.000223\n",
      "\t Validation Loss 0.000243\n",
      "Epoch 3530, Training Loss 0.000222\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 3540, Training Loss 0.000222\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 3550, Training Loss 0.000222\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 3560, Training Loss 0.000222\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 3570, Training Loss 0.000222\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 3580, Training Loss 0.000222\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 3590, Training Loss 0.000222\n",
      "\t Validation Loss 0.000240\n",
      "Epoch 3600, Training Loss 0.000222\n",
      "\t Validation Loss 0.000240\n",
      "Epoch 3610, Training Loss 0.000221\n",
      "\t Validation Loss 0.000240\n",
      "Epoch 3620, Training Loss 0.000221\n",
      "\t Validation Loss 0.000240\n",
      "Epoch 3630, Training Loss 0.000221\n",
      "\t Validation Loss 0.000240\n",
      "Epoch 3640, Training Loss 0.000221\n",
      "\t Validation Loss 0.000240\n",
      "Epoch 3650, Training Loss 0.000222\n",
      "\t Validation Loss 0.000242\n",
      "Epoch 3660, Training Loss 0.000316\n",
      "\t Validation Loss 0.000344\n",
      "Epoch 3670, Training Loss 0.000295\n",
      "\t Validation Loss 0.000302\n",
      "Epoch 3680, Training Loss 0.000222\n",
      "\t Validation Loss 0.000238\n",
      "Epoch 3690, Training Loss 0.000225\n",
      "\t Validation Loss 0.000246\n",
      "Epoch 3700, Training Loss 0.000224\n",
      "\t Validation Loss 0.000245\n",
      "Epoch 3710, Training Loss 0.000221\n",
      "\t Validation Loss 0.000239\n",
      "Epoch 3720, Training Loss 0.000220\n",
      "\t Validation Loss 0.000238\n",
      "Epoch 3730, Training Loss 0.000220\n",
      "\t Validation Loss 0.000239\n",
      "Epoch 3740, Training Loss 0.000220\n",
      "\t Validation Loss 0.000238\n",
      "Epoch 3750, Training Loss 0.000220\n",
      "\t Validation Loss 0.000238\n",
      "Epoch 3760, Training Loss 0.000219\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 3770, Training Loss 0.000219\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 3780, Training Loss 0.000219\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 3790, Training Loss 0.000219\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 3800, Training Loss 0.000219\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 3810, Training Loss 0.000219\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 3820, Training Loss 0.000219\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 3830, Training Loss 0.000218\n",
      "\t Validation Loss 0.000236\n",
      "Epoch 3840, Training Loss 0.000219\n",
      "\t Validation Loss 0.000236\n",
      "Epoch 3850, Training Loss 0.000243\n",
      "\t Validation Loss 0.000256\n",
      "Epoch 3860, Training Loss 0.000265\n",
      "\t Validation Loss 0.000274\n",
      "Epoch 3870, Training Loss 0.000245\n",
      "\t Validation Loss 0.000255\n",
      "Epoch 3880, Training Loss 0.000220\n",
      "\t Validation Loss 0.000238\n",
      "Epoch 3890, Training Loss 0.000222\n",
      "\t Validation Loss 0.000242\n",
      "Epoch 3900, Training Loss 0.000219\n",
      "\t Validation Loss 0.000236\n",
      "Epoch 3910, Training Loss 0.000218\n",
      "\t Validation Loss 0.000236\n",
      "Epoch 3920, Training Loss 0.000217\n",
      "\t Validation Loss 0.000235\n",
      "Epoch 3930, Training Loss 0.000217\n",
      "\t Validation Loss 0.000235\n",
      "Epoch 3940, Training Loss 0.000217\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 3950, Training Loss 0.000217\n",
      "\t Validation Loss 0.000235\n",
      "Epoch 3960, Training Loss 0.000217\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 3970, Training Loss 0.000217\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 3980, Training Loss 0.000217\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 3990, Training Loss 0.000216\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 4000, Training Loss 0.000216\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 4010, Training Loss 0.000216\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 4020, Training Loss 0.000231\n",
      "\t Validation Loss 0.000252\n",
      "Epoch 4030, Training Loss 0.000344\n",
      "\t Validation Loss 0.000373\n",
      "Epoch 4040, Training Loss 0.000240\n",
      "\t Validation Loss 0.000261\n",
      "Epoch 4050, Training Loss 0.000217\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 4060, Training Loss 0.000223\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 4070, Training Loss 0.000215\n",
      "\t Validation Loss 0.000233\n",
      "Epoch 4080, Training Loss 0.000216\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 4090, Training Loss 0.000216\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 4100, Training Loss 0.000215\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 4110, Training Loss 0.000215\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 4120, Training Loss 0.000215\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 4130, Training Loss 0.000215\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4140, Training Loss 0.000215\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4150, Training Loss 0.000214\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4160, Training Loss 0.000214\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4170, Training Loss 0.000214\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4180, Training Loss 0.000214\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4190, Training Loss 0.000214\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 4200, Training Loss 0.000214\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 4210, Training Loss 0.000221\n",
      "\t Validation Loss 0.000234\n",
      "Epoch 4220, Training Loss 0.000475\n",
      "\t Validation Loss 0.000473\n",
      "Epoch 4230, Training Loss 0.000229\n",
      "\t Validation Loss 0.000240\n",
      "Epoch 4240, Training Loss 0.000215\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4250, Training Loss 0.000216\n",
      "\t Validation Loss 0.000235\n",
      "Epoch 4260, Training Loss 0.000216\n",
      "\t Validation Loss 0.000235\n",
      "Epoch 4270, Training Loss 0.000213\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4280, Training Loss 0.000213\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4290, Training Loss 0.000213\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4300, Training Loss 0.000212\n",
      "\t Validation Loss 0.000228\n",
      "Epoch 4310, Training Loss 0.000212\n",
      "\t Validation Loss 0.000228\n",
      "Epoch 4320, Training Loss 0.000212\n",
      "\t Validation Loss 0.000228\n",
      "Epoch 4330, Training Loss 0.000212\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4340, Training Loss 0.000212\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4350, Training Loss 0.000212\n",
      "\t Validation Loss 0.000228\n",
      "Epoch 4360, Training Loss 0.000212\n",
      "\t Validation Loss 0.000228\n",
      "Epoch 4370, Training Loss 0.000213\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4380, Training Loss 0.000227\n",
      "\t Validation Loss 0.000247\n",
      "Epoch 4390, Training Loss 0.000217\n",
      "\t Validation Loss 0.000235\n",
      "Epoch 4400, Training Loss 0.000214\n",
      "\t Validation Loss 0.000228\n",
      "Epoch 4410, Training Loss 0.000218\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4420, Training Loss 0.000230\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 4430, Training Loss 0.000218\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 4440, Training Loss 0.000215\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 4450, Training Loss 0.000214\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4460, Training Loss 0.000215\n",
      "\t Validation Loss 0.000233\n",
      "Epoch 4470, Training Loss 0.000237\n",
      "\t Validation Loss 0.000257\n",
      "Epoch 4480, Training Loss 0.000218\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 4490, Training Loss 0.000218\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 4500, Training Loss 0.000212\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4510, Training Loss 0.000214\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 4520, Training Loss 0.000217\n",
      "\t Validation Loss 0.000235\n",
      "Epoch 4530, Training Loss 0.000223\n",
      "\t Validation Loss 0.000242\n",
      "Epoch 4540, Training Loss 0.000214\n",
      "\t Validation Loss 0.000231\n",
      "Epoch 4550, Training Loss 0.000209\n",
      "\t Validation Loss 0.000224\n",
      "Epoch 4560, Training Loss 0.000213\n",
      "\t Validation Loss 0.000226\n",
      "Epoch 4570, Training Loss 0.000263\n",
      "\t Validation Loss 0.000270\n",
      "Epoch 4580, Training Loss 0.000211\n",
      "\t Validation Loss 0.000226\n",
      "Epoch 4590, Training Loss 0.000214\n",
      "\t Validation Loss 0.000226\n",
      "Epoch 4600, Training Loss 0.000211\n",
      "\t Validation Loss 0.000228\n",
      "Epoch 4610, Training Loss 0.000208\n",
      "\t Validation Loss 0.000223\n",
      "Epoch 4620, Training Loss 0.000209\n",
      "\t Validation Loss 0.000223\n",
      "Epoch 4630, Training Loss 0.000208\n",
      "\t Validation Loss 0.000223\n",
      "Epoch 4640, Training Loss 0.000208\n",
      "\t Validation Loss 0.000224\n",
      "Epoch 4650, Training Loss 0.000208\n",
      "\t Validation Loss 0.000224\n",
      "Epoch 4660, Training Loss 0.000212\n",
      "\t Validation Loss 0.000229\n",
      "Epoch 4670, Training Loss 0.000267\n",
      "\t Validation Loss 0.000290\n",
      "Epoch 4680, Training Loss 0.000214\n",
      "\t Validation Loss 0.000225\n",
      "Epoch 4690, Training Loss 0.000213\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 4700, Training Loss 0.000209\n",
      "\t Validation Loss 0.000222\n",
      "Epoch 4710, Training Loss 0.000208\n",
      "\t Validation Loss 0.000222\n",
      "Epoch 4720, Training Loss 0.000208\n",
      "\t Validation Loss 0.000223\n",
      "Epoch 4730, Training Loss 0.000207\n",
      "\t Validation Loss 0.000223\n",
      "Epoch 4740, Training Loss 0.000207\n",
      "\t Validation Loss 0.000222\n",
      "Epoch 4750, Training Loss 0.000207\n",
      "\t Validation Loss 0.000222\n",
      "Epoch 4760, Training Loss 0.000213\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 4770, Training Loss 0.000310\n",
      "\t Validation Loss 0.000335\n",
      "Epoch 4780, Training Loss 0.000241\n",
      "\t Validation Loss 0.000248\n",
      "Epoch 4790, Training Loss 0.000211\n",
      "\t Validation Loss 0.000227\n",
      "Epoch 4800, Training Loss 0.000207\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 4810, Training Loss 0.000207\n",
      "\t Validation Loss 0.000223\n",
      "Epoch 4820, Training Loss 0.000207\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 4830, Training Loss 0.000206\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 4840, Training Loss 0.000206\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 4850, Training Loss 0.000205\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 4860, Training Loss 0.000205\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 4870, Training Loss 0.000206\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 4880, Training Loss 0.000220\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 4890, Training Loss 0.000291\n",
      "\t Validation Loss 0.000294\n",
      "Epoch 4900, Training Loss 0.000210\n",
      "\t Validation Loss 0.000225\n",
      "Epoch 4910, Training Loss 0.000207\n",
      "\t Validation Loss 0.000222\n",
      "Epoch 4920, Training Loss 0.000206\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 4930, Training Loss 0.000204\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 4940, Training Loss 0.000205\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 4950, Training Loss 0.000205\n",
      "\t Validation Loss 0.000217\n",
      "Epoch 4960, Training Loss 0.000204\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 4970, Training Loss 0.000204\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 4980, Training Loss 0.000204\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 4990, Training Loss 0.000205\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 5000, Training Loss 0.000231\n",
      "\t Validation Loss 0.000249\n",
      "Epoch 5010, Training Loss 0.000226\n",
      "\t Validation Loss 0.000244\n",
      "Epoch 5020, Training Loss 0.000204\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 5030, Training Loss 0.000204\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 5040, Training Loss 0.000204\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 5050, Training Loss 0.000205\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 5060, Training Loss 0.000203\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 5070, Training Loss 0.000203\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 5080, Training Loss 0.000203\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 5090, Training Loss 0.000203\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 5100, Training Loss 0.000203\n",
      "\t Validation Loss 0.000215\n",
      "Epoch 5110, Training Loss 0.000202\n",
      "\t Validation Loss 0.000215\n",
      "Epoch 5120, Training Loss 0.000202\n",
      "\t Validation Loss 0.000215\n",
      "Epoch 5130, Training Loss 0.000210\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 5140, Training Loss 0.000420\n",
      "\t Validation Loss 0.000415\n",
      "Epoch 5150, Training Loss 0.000270\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 5160, Training Loss 0.000217\n",
      "\t Validation Loss 0.000224\n",
      "Epoch 5170, Training Loss 0.000210\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 5180, Training Loss 0.000205\n",
      "\t Validation Loss 0.000217\n",
      "Epoch 5190, Training Loss 0.000202\n",
      "\t Validation Loss 0.000214\n",
      "Epoch 5200, Training Loss 0.000202\n",
      "\t Validation Loss 0.000215\n",
      "Epoch 5210, Training Loss 0.000201\n",
      "\t Validation Loss 0.000214\n",
      "Epoch 5220, Training Loss 0.000201\n",
      "\t Validation Loss 0.000214\n",
      "Epoch 5230, Training Loss 0.000201\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5240, Training Loss 0.000201\n",
      "\t Validation Loss 0.000214\n",
      "Epoch 5250, Training Loss 0.000201\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5260, Training Loss 0.000201\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5270, Training Loss 0.000201\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5280, Training Loss 0.000201\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5290, Training Loss 0.000201\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5300, Training Loss 0.000200\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5310, Training Loss 0.000200\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 5320, Training Loss 0.000200\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 5330, Training Loss 0.000200\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 5340, Training Loss 0.000200\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 5350, Training Loss 0.000200\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 5360, Training Loss 0.000200\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 5370, Training Loss 0.000201\n",
      "\t Validation Loss 0.000211\n",
      "Epoch 5380, Training Loss 0.000276\n",
      "\t Validation Loss 0.000278\n",
      "Epoch 5390, Training Loss 0.000254\n",
      "\t Validation Loss 0.000273\n",
      "Epoch 5400, Training Loss 0.000201\n",
      "\t Validation Loss 0.000211\n",
      "Epoch 5410, Training Loss 0.000210\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 5420, Training Loss 0.000199\n",
      "\t Validation Loss 0.000211\n",
      "Epoch 5430, Training Loss 0.000200\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5440, Training Loss 0.000200\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 5450, Training Loss 0.000199\n",
      "\t Validation Loss 0.000211\n",
      "Epoch 5460, Training Loss 0.000199\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 5470, Training Loss 0.000199\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 5480, Training Loss 0.000199\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 5490, Training Loss 0.000199\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 5500, Training Loss 0.000198\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 5510, Training Loss 0.000198\n",
      "\t Validation Loss 0.000209\n",
      "Epoch 5520, Training Loss 0.000198\n",
      "\t Validation Loss 0.000209\n",
      "Epoch 5530, Training Loss 0.000198\n",
      "\t Validation Loss 0.000209\n",
      "Epoch 5540, Training Loss 0.000198\n",
      "\t Validation Loss 0.000209\n",
      "Epoch 5550, Training Loss 0.000201\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 5560, Training Loss 0.000337\n",
      "\t Validation Loss 0.000335\n",
      "Epoch 5570, Training Loss 0.000257\n",
      "\t Validation Loss 0.000275\n",
      "Epoch 5580, Training Loss 0.000208\n",
      "\t Validation Loss 0.000222\n",
      "Epoch 5590, Training Loss 0.000203\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 5600, Training Loss 0.000198\n",
      "\t Validation Loss 0.000209\n",
      "Epoch 5610, Training Loss 0.000198\n",
      "\t Validation Loss 0.000209\n",
      "Epoch 5620, Training Loss 0.000198\n",
      "\t Validation Loss 0.000208\n",
      "Epoch 5630, Training Loss 0.000197\n",
      "\t Validation Loss 0.000208\n",
      "Epoch 5640, Training Loss 0.000197\n",
      "\t Validation Loss 0.000208\n",
      "Epoch 5650, Training Loss 0.000197\n",
      "\t Validation Loss 0.000208\n",
      "Epoch 5660, Training Loss 0.000197\n",
      "\t Validation Loss 0.000208\n",
      "Epoch 5670, Training Loss 0.000197\n",
      "\t Validation Loss 0.000207\n",
      "Epoch 5680, Training Loss 0.000197\n",
      "\t Validation Loss 0.000207\n",
      "Epoch 5690, Training Loss 0.000197\n",
      "\t Validation Loss 0.000207\n",
      "Epoch 5700, Training Loss 0.000197\n",
      "\t Validation Loss 0.000207\n",
      "Epoch 5710, Training Loss 0.000196\n",
      "\t Validation Loss 0.000207\n",
      "Epoch 5720, Training Loss 0.000200\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 5730, Training Loss 0.000381\n",
      "\t Validation Loss 0.000404\n",
      "Epoch 5740, Training Loss 0.000213\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 5750, Training Loss 0.000218\n",
      "\t Validation Loss 0.000222\n",
      "Epoch 5760, Training Loss 0.000197\n",
      "\t Validation Loss 0.000206\n",
      "Epoch 5770, Training Loss 0.000199\n",
      "\t Validation Loss 0.000211\n",
      "Epoch 5780, Training Loss 0.000197\n",
      "\t Validation Loss 0.000206\n",
      "Epoch 5790, Training Loss 0.000196\n",
      "\t Validation Loss 0.000206\n",
      "Epoch 5800, Training Loss 0.000196\n",
      "\t Validation Loss 0.000206\n",
      "Epoch 5810, Training Loss 0.000196\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5820, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5830, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5840, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5850, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5860, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5870, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5880, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5890, Training Loss 0.000195\n",
      "\t Validation Loss 0.000204\n",
      "Epoch 5900, Training Loss 0.000201\n",
      "\t Validation Loss 0.000208\n",
      "Epoch 5910, Training Loss 0.000398\n",
      "\t Validation Loss 0.000391\n",
      "Epoch 5920, Training Loss 0.000196\n",
      "\t Validation Loss 0.000204\n",
      "Epoch 5930, Training Loss 0.000205\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 5940, Training Loss 0.000199\n",
      "\t Validation Loss 0.000211\n",
      "Epoch 5950, Training Loss 0.000198\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5960, Training Loss 0.000195\n",
      "\t Validation Loss 0.000205\n",
      "Epoch 5970, Training Loss 0.000194\n",
      "\t Validation Loss 0.000204\n",
      "Epoch 5980, Training Loss 0.000194\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 5990, Training Loss 0.000194\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6000, Training Loss 0.000194\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6010, Training Loss 0.000194\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6020, Training Loss 0.000194\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6030, Training Loss 0.000194\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6040, Training Loss 0.000194\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6050, Training Loss 0.000193\n",
      "\t Validation Loss 0.000202\n",
      "Epoch 6060, Training Loss 0.000193\n",
      "\t Validation Loss 0.000202\n",
      "Epoch 6070, Training Loss 0.000193\n",
      "\t Validation Loss 0.000202\n",
      "Epoch 6080, Training Loss 0.000195\n",
      "\t Validation Loss 0.000202\n",
      "Epoch 6090, Training Loss 0.000317\n",
      "\t Validation Loss 0.000314\n",
      "Epoch 6100, Training Loss 0.000270\n",
      "\t Validation Loss 0.000288\n",
      "Epoch 6110, Training Loss 0.000214\n",
      "\t Validation Loss 0.000227\n",
      "Epoch 6120, Training Loss 0.000193\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6130, Training Loss 0.000197\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6140, Training Loss 0.000194\n",
      "\t Validation Loss 0.000204\n",
      "Epoch 6150, Training Loss 0.000193\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6160, Training Loss 0.000193\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6170, Training Loss 0.000193\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6180, Training Loss 0.000192\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6190, Training Loss 0.000192\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6200, Training Loss 0.000192\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6210, Training Loss 0.000192\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6220, Training Loss 0.000192\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 6230, Training Loss 0.000192\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 6240, Training Loss 0.000192\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 6250, Training Loss 0.000192\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 6260, Training Loss 0.000192\n",
      "\t Validation Loss 0.000202\n",
      "Epoch 6270, Training Loss 0.000229\n",
      "\t Validation Loss 0.000243\n",
      "Epoch 6280, Training Loss 0.000192\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 6290, Training Loss 0.000219\n",
      "\t Validation Loss 0.000232\n",
      "Epoch 6300, Training Loss 0.000192\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6310, Training Loss 0.000195\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 6320, Training Loss 0.000193\n",
      "\t Validation Loss 0.000203\n",
      "Epoch 6330, Training Loss 0.000192\n",
      "\t Validation Loss 0.000199\n",
      "Epoch 6340, Training Loss 0.000191\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 6350, Training Loss 0.000191\n",
      "\t Validation Loss 0.000199\n",
      "Epoch 6360, Training Loss 0.000191\n",
      "\t Validation Loss 0.000199\n",
      "Epoch 6370, Training Loss 0.000191\n",
      "\t Validation Loss 0.000199\n",
      "Epoch 6380, Training Loss 0.000191\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6390, Training Loss 0.000191\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6400, Training Loss 0.000191\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6410, Training Loss 0.000191\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6420, Training Loss 0.000191\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6430, Training Loss 0.000200\n",
      "\t Validation Loss 0.000204\n",
      "Epoch 6440, Training Loss 0.000354\n",
      "\t Validation Loss 0.000347\n",
      "Epoch 6450, Training Loss 0.000227\n",
      "\t Validation Loss 0.000227\n",
      "Epoch 6460, Training Loss 0.000193\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6470, Training Loss 0.000197\n",
      "\t Validation Loss 0.000207\n",
      "Epoch 6480, Training Loss 0.000190\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6490, Training Loss 0.000191\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6500, Training Loss 0.000190\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6510, Training Loss 0.000190\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6520, Training Loss 0.000190\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6530, Training Loss 0.000190\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6540, Training Loss 0.000190\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6550, Training Loss 0.000190\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6560, Training Loss 0.000190\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6570, Training Loss 0.000189\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 6580, Training Loss 0.000189\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 6590, Training Loss 0.000189\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 6600, Training Loss 0.000189\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 6610, Training Loss 0.000189\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 6620, Training Loss 0.000194\n",
      "\t Validation Loss 0.000199\n",
      "Epoch 6630, Training Loss 0.000385\n",
      "\t Validation Loss 0.000375\n",
      "Epoch 6640, Training Loss 0.000191\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6650, Training Loss 0.000203\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 6660, Training Loss 0.000191\n",
      "\t Validation Loss 0.000199\n",
      "Epoch 6670, Training Loss 0.000192\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6680, Training Loss 0.000190\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 6690, Training Loss 0.000189\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6700, Training Loss 0.000189\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6710, Training Loss 0.000189\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6720, Training Loss 0.000188\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6730, Training Loss 0.000188\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6740, Training Loss 0.000188\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6750, Training Loss 0.000188\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6760, Training Loss 0.000188\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 6770, Training Loss 0.000188\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 6780, Training Loss 0.000188\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 6790, Training Loss 0.000189\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 6800, Training Loss 0.000254\n",
      "\t Validation Loss 0.000251\n",
      "Epoch 6810, Training Loss 0.000218\n",
      "\t Validation Loss 0.000230\n",
      "Epoch 6820, Training Loss 0.000195\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 6830, Training Loss 0.000193\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 6840, Training Loss 0.000192\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 6850, Training Loss 0.000189\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 6860, Training Loss 0.000188\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6870, Training Loss 0.000188\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 6880, Training Loss 0.000187\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 6890, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 6900, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 6910, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 6920, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 6930, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 6940, Training Loss 0.000188\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 6950, Training Loss 0.000233\n",
      "\t Validation Loss 0.000246\n",
      "Epoch 6960, Training Loss 0.000191\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 6970, Training Loss 0.000209\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 6980, Training Loss 0.000188\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 6990, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 7000, Training Loss 0.000187\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7010, Training Loss 0.000187\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7020, Training Loss 0.000187\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7030, Training Loss 0.000187\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7040, Training Loss 0.000186\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7050, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7060, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7070, Training Loss 0.000187\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7080, Training Loss 0.000194\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 7090, Training Loss 0.000295\n",
      "\t Validation Loss 0.000288\n",
      "Epoch 7100, Training Loss 0.000202\n",
      "\t Validation Loss 0.000211\n",
      "Epoch 7110, Training Loss 0.000187\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7120, Training Loss 0.000187\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7130, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7140, Training Loss 0.000186\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7150, Training Loss 0.000186\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 7160, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7170, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7180, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7190, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7200, Training Loss 0.000189\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 7210, Training Loss 0.000268\n",
      "\t Validation Loss 0.000282\n",
      "Epoch 7220, Training Loss 0.000219\n",
      "\t Validation Loss 0.000217\n",
      "Epoch 7230, Training Loss 0.000195\n",
      "\t Validation Loss 0.000202\n",
      "Epoch 7240, Training Loss 0.000187\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 7250, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 7260, Training Loss 0.000186\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 7270, Training Loss 0.000185\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 7280, Training Loss 0.000185\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 7290, Training Loss 0.000185\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 7300, Training Loss 0.000185\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 7310, Training Loss 0.000185\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 7320, Training Loss 0.000187\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 7330, Training Loss 0.000245\n",
      "\t Validation Loss 0.000241\n",
      "Epoch 7340, Training Loss 0.000198\n",
      "\t Validation Loss 0.000206\n",
      "Epoch 7350, Training Loss 0.000201\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 7360, Training Loss 0.000191\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 7370, Training Loss 0.000188\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 7380, Training Loss 0.000185\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 7390, Training Loss 0.000184\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 7400, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7410, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7420, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7430, Training Loss 0.000184\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 7440, Training Loss 0.000188\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 7450, Training Loss 0.000286\n",
      "\t Validation Loss 0.000301\n",
      "Epoch 7460, Training Loss 0.000222\n",
      "\t Validation Loss 0.000218\n",
      "Epoch 7470, Training Loss 0.000185\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 7480, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7490, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7500, Training Loss 0.000184\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 7510, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7520, Training Loss 0.000184\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 7530, Training Loss 0.000184\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 7540, Training Loss 0.000183\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 7550, Training Loss 0.000183\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 7560, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7570, Training Loss 0.000201\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 7580, Training Loss 0.000228\n",
      "\t Validation Loss 0.000239\n",
      "Epoch 7590, Training Loss 0.000188\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 7600, Training Loss 0.000194\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 7610, Training Loss 0.000188\n",
      "\t Validation Loss 0.000194\n",
      "Epoch 7620, Training Loss 0.000185\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 7630, Training Loss 0.000183\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7640, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7650, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7660, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7670, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7680, Training Loss 0.000183\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 7690, Training Loss 0.000187\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 7700, Training Loss 0.000259\n",
      "\t Validation Loss 0.000271\n",
      "Epoch 7710, Training Loss 0.000209\n",
      "\t Validation Loss 0.000206\n",
      "Epoch 7720, Training Loss 0.000194\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 7730, Training Loss 0.000189\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 7740, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7750, Training Loss 0.000182\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7760, Training Loss 0.000183\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 7770, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 7780, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 7790, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7800, Training Loss 0.000194\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 7810, Training Loss 0.000253\n",
      "\t Validation Loss 0.000265\n",
      "Epoch 7820, Training Loss 0.000188\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7830, Training Loss 0.000182\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 7840, Training Loss 0.000182\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 7850, Training Loss 0.000183\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7860, Training Loss 0.000182\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 7870, Training Loss 0.000182\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 7880, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 7890, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 7900, Training Loss 0.000184\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 7910, Training Loss 0.000211\n",
      "\t Validation Loss 0.000220\n",
      "Epoch 7920, Training Loss 0.000187\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 7930, Training Loss 0.000182\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 7940, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7950, Training Loss 0.000185\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 7960, Training Loss 0.000182\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 7970, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 7980, Training Loss 0.000182\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 7990, Training Loss 0.000187\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 8000, Training Loss 0.000228\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 8010, Training Loss 0.000187\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 8020, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 8030, Training Loss 0.000185\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 8040, Training Loss 0.000181\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 8050, Training Loss 0.000182\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 8060, Training Loss 0.000185\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 8070, Training Loss 0.000202\n",
      "\t Validation Loss 0.000199\n",
      "Epoch 8080, Training Loss 0.000192\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 8090, Training Loss 0.000190\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 8100, Training Loss 0.000181\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8110, Training Loss 0.000187\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 8120, Training Loss 0.000199\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 8130, Training Loss 0.000181\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8140, Training Loss 0.000185\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 8150, Training Loss 0.000192\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 8160, Training Loss 0.000194\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 8170, Training Loss 0.000181\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8180, Training Loss 0.000186\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 8190, Training Loss 0.000204\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 8200, Training Loss 0.000182\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8210, Training Loss 0.000186\n",
      "\t Validation Loss 0.000191\n",
      "Epoch 8220, Training Loss 0.000180\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 8230, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8240, Training Loss 0.000182\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8250, Training Loss 0.000218\n",
      "\t Validation Loss 0.000212\n",
      "Epoch 8260, Training Loss 0.000180\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8270, Training Loss 0.000185\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 8280, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 8290, Training Loss 0.000180\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 8300, Training Loss 0.000181\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8310, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8320, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8330, Training Loss 0.000180\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 8340, Training Loss 0.000185\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 8350, Training Loss 0.000228\n",
      "\t Validation Loss 0.000237\n",
      "Epoch 8360, Training Loss 0.000187\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 8370, Training Loss 0.000181\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 8380, Training Loss 0.000182\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 8390, Training Loss 0.000180\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 8400, Training Loss 0.000181\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8410, Training Loss 0.000182\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8420, Training Loss 0.000192\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 8430, Training Loss 0.000206\n",
      "\t Validation Loss 0.000201\n",
      "Epoch 8440, Training Loss 0.000186\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 8450, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8460, Training Loss 0.000180\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 8470, Training Loss 0.000190\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 8480, Training Loss 0.000202\n",
      "\t Validation Loss 0.000197\n",
      "Epoch 8490, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 8500, Training Loss 0.000182\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 8510, Training Loss 0.000179\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8520, Training Loss 0.000179\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8530, Training Loss 0.000188\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 8540, Training Loss 0.000246\n",
      "\t Validation Loss 0.000256\n",
      "Epoch 8550, Training Loss 0.000197\n",
      "\t Validation Loss 0.000192\n",
      "Epoch 8560, Training Loss 0.000185\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 8570, Training Loss 0.000184\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8580, Training Loss 0.000179\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 8590, Training Loss 0.000179\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8600, Training Loss 0.000178\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 8610, Training Loss 0.000179\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 8620, Training Loss 0.000181\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 8630, Training Loss 0.000219\n",
      "\t Validation Loss 0.000213\n",
      "Epoch 8640, Training Loss 0.000179\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8650, Training Loss 0.000179\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 8660, Training Loss 0.000180\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 8670, Training Loss 0.000180\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 8680, Training Loss 0.000179\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8690, Training Loss 0.000178\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 8700, Training Loss 0.000178\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 8710, Training Loss 0.000184\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8720, Training Loss 0.000257\n",
      "\t Validation Loss 0.000247\n",
      "Epoch 8730, Training Loss 0.000204\n",
      "\t Validation Loss 0.000209\n",
      "Epoch 8740, Training Loss 0.000188\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 8750, Training Loss 0.000184\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 8760, Training Loss 0.000178\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 8770, Training Loss 0.000178\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 8780, Training Loss 0.000178\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 8790, Training Loss 0.000178\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 8800, Training Loss 0.000179\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8810, Training Loss 0.000191\n",
      "\t Validation Loss 0.000196\n",
      "Epoch 8820, Training Loss 0.000212\n",
      "\t Validation Loss 0.000219\n",
      "Epoch 8830, Training Loss 0.000191\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 8840, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 8850, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8860, Training Loss 0.000178\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 8870, Training Loss 0.000179\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 8880, Training Loss 0.000183\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8890, Training Loss 0.000215\n",
      "\t Validation Loss 0.000208\n",
      "Epoch 8900, Training Loss 0.000180\n",
      "\t Validation Loss 0.000181\n",
      "Epoch 8910, Training Loss 0.000178\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 8920, Training Loss 0.000182\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 8930, Training Loss 0.000185\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 8940, Training Loss 0.000178\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 8950, Training Loss 0.000178\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 8960, Training Loss 0.000185\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 8970, Training Loss 0.000234\n",
      "\t Validation Loss 0.000242\n",
      "Epoch 8980, Training Loss 0.000195\n",
      "\t Validation Loss 0.000190\n",
      "Epoch 8990, Training Loss 0.000186\n",
      "\t Validation Loss 0.000189\n",
      "Epoch 9000, Training Loss 0.000177\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9010, Training Loss 0.000179\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9020, Training Loss 0.000177\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9030, Training Loss 0.000177\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 9040, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 9050, Training Loss 0.000215\n",
      "\t Validation Loss 0.000221\n",
      "Epoch 9060, Training Loss 0.000177\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9070, Training Loss 0.000177\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9080, Training Loss 0.000182\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 9090, Training Loss 0.000177\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9100, Training Loss 0.000178\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9110, Training Loss 0.000178\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9120, Training Loss 0.000183\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 9130, Training Loss 0.000214\n",
      "\t Validation Loss 0.000206\n",
      "Epoch 9140, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 9150, Training Loss 0.000177\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 9160, Training Loss 0.000180\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 9170, Training Loss 0.000186\n",
      "\t Validation Loss 0.000182\n",
      "Epoch 9180, Training Loss 0.000178\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9190, Training Loss 0.000177\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9200, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 9210, Training Loss 0.000231\n",
      "\t Validation Loss 0.000238\n",
      "Epoch 9220, Training Loss 0.000194\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 9230, Training Loss 0.000184\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 9240, Training Loss 0.000176\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9250, Training Loss 0.000178\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9260, Training Loss 0.000176\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9270, Training Loss 0.000176\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9280, Training Loss 0.000176\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9290, Training Loss 0.000183\n",
      "\t Validation Loss 0.000186\n",
      "Epoch 9300, Training Loss 0.000263\n",
      "\t Validation Loss 0.000273\n",
      "Epoch 9310, Training Loss 0.000188\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 9320, Training Loss 0.000176\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9330, Training Loss 0.000177\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9340, Training Loss 0.000177\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 9350, Training Loss 0.000176\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9360, Training Loss 0.000175\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9370, Training Loss 0.000176\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9380, Training Loss 0.000175\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9390, Training Loss 0.000175\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9400, Training Loss 0.000177\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 9410, Training Loss 0.000232\n",
      "\t Validation Loss 0.000239\n",
      "Epoch 9420, Training Loss 0.000193\n",
      "\t Validation Loss 0.000187\n",
      "Epoch 9430, Training Loss 0.000192\n",
      "\t Validation Loss 0.000195\n",
      "Epoch 9440, Training Loss 0.000183\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 9450, Training Loss 0.000177\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 9460, Training Loss 0.000175\n",
      "\t Validation Loss 0.000174\n",
      "Epoch 9470, Training Loss 0.000175\n",
      "\t Validation Loss 0.000174\n",
      "Epoch 9480, Training Loss 0.000175\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9490, Training Loss 0.000175\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9500, Training Loss 0.000175\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9510, Training Loss 0.000177\n",
      "\t Validation Loss 0.000178\n",
      "Epoch 9520, Training Loss 0.000218\n",
      "\t Validation Loss 0.000224\n",
      "Epoch 9530, Training Loss 0.000179\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9540, Training Loss 0.000183\n",
      "\t Validation Loss 0.000185\n",
      "Epoch 9550, Training Loss 0.000176\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9560, Training Loss 0.000176\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9570, Training Loss 0.000176\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9580, Training Loss 0.000175\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9590, Training Loss 0.000175\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9600, Training Loss 0.000176\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9610, Training Loss 0.000185\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 9620, Training Loss 0.000219\n",
      "\t Validation Loss 0.000210\n",
      "Epoch 9630, Training Loss 0.000190\n",
      "\t Validation Loss 0.000193\n",
      "Epoch 9640, Training Loss 0.000180\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9650, Training Loss 0.000177\n",
      "\t Validation Loss 0.000174\n",
      "Epoch 9660, Training Loss 0.000175\n",
      "\t Validation Loss 0.000175\n",
      "Epoch 9670, Training Loss 0.000176\n",
      "\t Validation Loss 0.000176\n",
      "Epoch 9680, Training Loss 0.000178\n",
      "\t Validation Loss 0.000179\n",
      "Epoch 9690, Training Loss 0.000194\n",
      "\t Validation Loss 0.000198\n",
      "Epoch 9700, Training Loss 0.000179\n",
      "\t Validation Loss 0.000180\n",
      "Epoch 9710, Training Loss 0.000182\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9720, Training Loss 0.000181\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9730, Training Loss 0.000176\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9740, Training Loss 0.000174\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9750, Training Loss 0.000175\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9760, Training Loss 0.000195\n",
      "\t Validation Loss 0.000188\n",
      "Epoch 9770, Training Loss 0.000191\n",
      "\t Validation Loss 0.000184\n",
      "Epoch 9780, Training Loss 0.000178\n",
      "\t Validation Loss 0.000174\n",
      "Epoch 9790, Training Loss 0.000177\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9800, Training Loss 0.000174\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9810, Training Loss 0.000174\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9820, Training Loss 0.000174\n",
      "\t Validation Loss 0.000174\n",
      "Epoch 9830, Training Loss 0.000174\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9840, Training Loss 0.000174\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9850, Training Loss 0.000174\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9860, Training Loss 0.000174\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9870, Training Loss 0.000189\n",
      "\t Validation Loss 0.000183\n",
      "Epoch 9880, Training Loss 0.000209\n",
      "\t Validation Loss 0.000200\n",
      "Epoch 9890, Training Loss 0.000175\n",
      "\t Validation Loss 0.000174\n",
      "Epoch 9900, Training Loss 0.000175\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9910, Training Loss 0.000177\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9920, Training Loss 0.000175\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9930, Training Loss 0.000173\n",
      "\t Validation Loss 0.000172\n",
      "Epoch 9940, Training Loss 0.000174\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9950, Training Loss 0.000174\n",
      "\t Validation Loss 0.000173\n",
      "Epoch 9960, Training Loss 0.000177\n",
      "\t Validation Loss 0.000177\n",
      "Epoch 9970, Training Loss 0.000211\n",
      "\t Validation Loss 0.000216\n",
      "Epoch 9980, Training Loss 0.000175\n",
      "\t Validation Loss 0.000171\n",
      "Epoch 9990, Training Loss 0.000173\n",
      "\t Validation Loss 0.000171\n",
      "Epoch 10000, Training Loss 0.000180\n",
      "\t Validation Loss 0.000182\n"
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs=10000, optimizer=optimizer,model=model,\n",
    "             loss_fn=nn.MSELoss(), X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解と予測結果をプロットしてみましょう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predict=torch.sum(model(X_test),dim=1)/64.0\n",
    "    #plt.xlim((-6,-5.6))\n",
    "    #plt.ylim((-6,-5.6))\n",
    "    plt.scatter(y_test,predict)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1224aa78933d76f3b87bf99e45c31cce45160fda9d38f1d681cad51c427d002b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('dev_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
